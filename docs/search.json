[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Data Analytics Engineering",
    "section": "",
    "text": "Preface\nThis book documents the data analytics engineering workflow, which contains two parts namely infrastructure and tools. It focuses on its implementation instead of its setup. macOS is left out as Windows OS is widely used in the business setting. Pick the preferred tools after considered your career path. For instance, data/dev ops, data/analytic/ML engineer, and data analyst/scientist. My goal is to have a better solution to do auditing/accounting job easily (powerful tools), accurately (reproducible process), and automatically (job scheduler). If you don’t know what I am talking about, watch data firm, financial statement preparation, insurance data analysis, and read the paper (Li, Fisher, and Falta 2020).\nYou might ask how it relates to you. Generally, CFO is charge of COA, Audit partner emphasize accounting treatments, and staffs do their job at the transactional level. You need much better tools to pan out at work. For instance,\n1. New job requires the strong analytic mind. Excel or similar tools are not sufficient for pattern recognition.\n2. A higher staff turnover is caused by pressure and boredom. You need to be efficient by automating repetitive work such as reconciliation.\n\n\n\n\nLi, Stewart, Richard Fisher, and Michael Falta. 2020. “The Effectiveness of Artificial Neural Networks Applied to Analytical Procedures Using High Level Data: A Simulation Analysis.” Meditari Accountancy Research 29 (6): 1425–50. https://doi.org/10.1108/medar-06-2020-0920."
  },
  {
    "objectID": "infra.html",
    "href": "infra.html",
    "title": "Infrastructure",
    "section": "",
    "text": "The knowledge of linux (Ubuntu LTS) terminal will be beneficial when you use remote AWS services. For instance,\n1. awscli, terraform,\n2. docker, podman, k8,\nELT seems better than ETL as you normally don’t know the part of transformation upfront."
  },
  {
    "objectID": "infra_local.html",
    "href": "infra_local.html",
    "title": "1  Local",
    "section": "",
    "text": "My OS is Windows 11. Install Window manager komorebi, Windows Terminal wsl2, and Linux distribution systems. Edit terminal theme/font, dotfiles of Bash/Tmux/Vim, and env variables. Install Git/GitBash and Docker/Podman if needed.\n\n\n\nDesktop\n\n\n\n\n\nCMD\n\n\n\n\n\nPowerShell\n\n\n\n\n\nUbuntu\n\n\nInstall programming languages R/Python/DuckDB/Rust/Go. R is a language designed to get shit done (@hadleywickham). Python is a glue language. Rust is a decent language for software engineering. I often live in terminal to rofi applications, manage pass, rsync files, quarto markdown, sftp to server, ssh into remote machines, and do a quick analysis for ad hoc tasks.\n\n\n\nTerminal tools\n\n\n\n\n\nR, Python, DuckDB\n\n\nEditors like nano (Linux) and notepad (Windows) can be used for their simplicity. However, appropriate IDE helps you organize your project better. I choose Vim (Linux), RStudio (Windows), and VS Code (Both) based on the active development environment. Of course, RStudio can be launched in Linux as well.\n\n\n\nVim - R\n\n\n\n\n\nVim - Python\n\n\n\n\n\nTmux - Nvim 1\n\n\n\n\n\nTmux -Nvim 2\n\n\n\n\n\nTmux - Nvim 3\n\n\n\n\n\nTmux -Nvim 4\n\n\n\n\n\nRStudio - R\n\n\n\n\n\nRStudio - Python\n\n\n\n\n\nVS Code in Linux - Jupyter\n\n\n\n\n\nVS Code in Linux - Interactive cell\n\n\n\n\n\nVS Code in Windows - Script\n\n\n\n\n\nVS Code in Windows - Interactive cell\n\n\n\n\n\nVS Code in Windows - R\n\n\nIt is vital to create a proper folder structure along with config file as you are able to move quickly and organize your scripts better. I run a command line tool (written in R) from GitBash and PowerShell to do it.\n\n\n\nCLI R - GitBash 1\n\n\n\n\n\nCLI R - GitBash 2\n\n\n\n\n\nCLI R - GitBash 3\n\n\n\n\n\nCLI R - GitBash 4\n\n\n\n\n\nCLI R - GitBash 5\n\n\n\n\n\nCLI R - PowerShell 1\n\n\n\n\n\nCLI R - PowerShell 2"
  },
  {
    "objectID": "infra_etl.html",
    "href": "infra_etl.html",
    "title": "2  ELT",
    "section": "",
    "text": "Consider the following examples to establish a data pipeline.\n1. A zip file lands in data lake (s3/minio) daily.\n2. Execute scripts in the server (ec2) to download/unzip/select/upload files based on mtime. It produces a file (csv) to track work done at the agreed cut-off time (cron). AWS lambda is another option.\n3. snowflake external stage (s3) is triggered by a file (txt) to kicks off snowpipe and ingest data to DB as variant. Similar storage are databrick, dremio, clickhouse. The preferred formats are parquet, iceberg, ADBC.\n4. Move data between platforms via airbyte.\n5. Validate and transform DB raw to DB mart through dbt.\n6. Automatize the process by a task scheduler prefect, airflow, dagster.\n7. Create a dashboard for DB mart via metabase, superset.\n\n\n\nDuckDB cloud\n\n\n\n\n\nDuckDB terminal"
  },
  {
    "objectID": "infra_axum.html",
    "href": "infra_axum.html",
    "title": "3  HTTP",
    "section": "",
    "text": "It is very useful to create a micro service API internally.\n\n\n\nWeb server\n\n\n\n\n\nGet\n\n\n\n\n\nPost\n\n\n\n\n\nTemplate\n\n\n\n\n\nR client\n\n\n\nhttr2::request('localhost:3000/share') %&gt;% \n  httr2::req_perform() %&gt;% \n  httr2::resp_body_string()\n\n\n\n\nRequest CLI 1\n\n\n\n\n\nRequest CLI 2"
  },
  {
    "objectID": "infra_faudit.html",
    "href": "infra_faudit.html",
    "title": "4  FAudit",
    "section": "",
    "text": "Create a command line tool to organize the workflow including folder structure and relevant config files.\n\n\n\n\nfaudit help\n\n\n\n\n\nfaudit init\n\n\n\n\n\nfaudit new 1\n\n\n\n\n\nfaudit new 2\n\n\n\n\n\nfaudit report\n\n\n\n\n\nfaudit new 3\n\n\n\n\n\nfaudit new 4\n\n\n\n\n\nfaudit show"
  },
  {
    "objectID": "dtool.html#footnotes",
    "href": "dtool.html#footnotes",
    "title": "Data tools",
    "section": "",
    "text": "ggplot2 (Wickham 2016)↩︎"
  },
  {
    "objectID": "dtool_polars.html",
    "href": "dtool_polars.html",
    "title": "5  Polars",
    "section": "",
    "text": "Command line tools allow you to do those repetitive data work easily. The following three examples are.\n1. argparse and duckdb.\n2. click and polars.\n3. clap and polars.\n\n\n\nCLI - argparse 1\n\n\n\n\n\nCLI - argparse 2\n\n\n\n\n\nCLI - argparse 3\n\n\n\n\n\nCLI - click 1\n\n\n\n\n\nCLI - click 2\n\n\n\n\n\nCLI - click 3\n\n\n\n\n\nCLI - click 4\n\n\n\n\n\nCLI - clap 1\n\n\n\n\n\nCLI - clap 2"
  },
  {
    "objectID": "dtool_datatable.html#io",
    "href": "dtool_datatable.html#io",
    "title": "6  Analysis",
    "section": "6.1 IO",
    "text": "6.1 IO\n\ndf_raw &lt;- read_csv(here::here('data/factor_ar.csv')) %&gt;% \n  janitor::clean_names() \n\nglimpse(df_raw)\n\ndata.table is the fastest IO tool if your data can fit in the memory.\n\nlibrary(data.table)\n\n# read in\ndata.table::fread(\"grep -v '770' ./data/factor_ar.csv\")[, .N, by = countryCode]\n\n# write out\ndf_dt &lt;- as.data.table(df_raw)\n\ndf_dt[, \n      fwrite(data.table(.SD), \n             paste0(\"C:/Users/Stewart Li/Desktop/res/\", \n                    paste0(country_code, \".csv\"))), by = country_code]\n\n# read in \ndata.table(\n  country_code.csv = Sys.glob(\"C:/Users/Stewart Li/Desktop/res/*.csv\")\n)[, fread(country_code.csv), by = country_code.csv]\n\nGet to know your data. For instance, any missing value, counting variables, and others.\n\n# no NA\nsapply(df_raw, function(x) {sum(is.na(x)) / nrow(df_raw)}) %&gt;%   \n  enframe() %&gt;%                                                              \n  mutate(value = formattable::percent(value))\n\nnaniar::gg_miss_var(df_raw)\nnaniar::vis_miss(df_raw)\n\n# no duplicate \ndf_raw %&gt;% count(invoice_number, sort = TRUE)                                \n\n# overview of data\nskimr::skim(df_raw)"
  },
  {
    "objectID": "dtool_datatable.html#cleaning",
    "href": "dtool_datatable.html#cleaning",
    "title": "6  Analysis",
    "section": "6.2 Cleaning",
    "text": "6.2 Cleaning\nAfter having a basic understanding about data, do the followings to clean it up.\n1. cast data types.\n2. 30 days credit term is allowed. drop it subsequently (constant).\n3. drop column (paperless_date).\n4. rename and rearrange columns.\n\ndf_clean &lt;- df_raw %&gt;%  \n  mutate(across(contains(\"date\"), lubridate::mdy), \n         across(c(country_code, invoice_number), as.character)) %&gt;% \n  mutate(credit = as.numeric(due_date - invoice_date)) %&gt;% \n  select(c(country_code, customer_id, paperless_bill, disputed, \n           invoice_number, invoice_amount, invoice_date, due_date, settled_date, \n           settle = days_to_settle, late = days_late))\n\nsetdiff(colnames(df_raw), colnames(df_clean))"
  },
  {
    "objectID": "dtool_datatable.html#validate",
    "href": "dtool_datatable.html#validate",
    "title": "6  Analysis",
    "section": "6.3 Validate",
    "text": "6.3 Validate\nValidate data if it is received from other team members.\n\n# data type\ndf_clean %&gt;% \n  select(contains(\"date\")) %&gt;% \n  pointblank::col_is_date(columns = everything())   \n\n# cross checking\ndf_clean %&gt;% \n  mutate(settle1 = as.numeric(settled_date - invoice_date), \n         late1 = as.numeric(settled_date - due_date), \n         late1 = if_else(late1 &lt; 0, 0, late1)) %&gt;% \n  summarise(late_sum = sum(late1) - sum(late), \n            settle_sum = sum(settle1) - sum(settle))"
  },
  {
    "objectID": "dtool_datatable.html#munging",
    "href": "dtool_datatable.html#munging",
    "title": "6  Analysis",
    "section": "6.4 Munging",
    "text": "6.4 Munging\nAsk reasonable questions via slice dice.\n\n# window operation: lag, first, nth, \ndf_clean %&gt;% \n  arrange(invoice_date) %&gt;% \n  group_by(country_code) %&gt;% \n  mutate(increase = invoice_amount - dplyr::lag(invoice_amount, default = 0), \n         indcator = ifelse(increase &gt; 0, 1, 0)) %&gt;% \n  ungroup() %&gt;% \n  mutate(settle_grp = (settle %/% 10) * 10) \n\ndf_clean %&gt;% \n  group_by(country_code) %&gt;% \n  arrange(invoice_date) %&gt;% \n  summarise(n = n(), \n            sales = sum(invoice_amount), \n            first_disputed_late = first(late[disputed == 'Yes']), \n            first_disputed_inv_date = first(invoice_date[disputed == 'Yes']), \n            largest_late = max(late[disputed == 'Yes']), \n            largest_inv_amt = invoice_amount[late == max(late)], \n            .groups = 'drop') \n\nCut late into four categories based on the firm’s credit policy.\n\nsort(unique(df_clean$late))\n\ndf_late &lt;- df_clean %&gt;% \n  dplyr::filter(late != 0) %&gt;%\n  mutate(reminder = case_when(late &gt; 0 & late &lt;= 10 ~ \"1st email\",\n                              late &gt; 10 & late &lt;= 20 ~ \"2nd email\",\n                              late &gt; 20 & late &lt;= 30 ~ \"legal case\",\n                              TRUE ~ \"bad debt\")) \n\n# anomaly by country\ndf_late %&gt;% \n  ggplot(aes(late, disputed, color = country_code)) +\n  geom_boxplot() +\n  theme_light()\n\n# summary table\ndf_late %&gt;% \n  group_by(reminder, disputed) %&gt;% \n  summarise(across(late, tibble::lst(sum, min, max, sd)), \n            .groups = 'drop') %&gt;% \n  gt::gt()\n\n# clients without dispute do not pay. \ndf_late %&gt;% \n  dplyr::filter(disputed == 'No', reminder %in% c('legal case', 'bad debt'))\n\n\n\n\nData munging"
  },
  {
    "objectID": "dtool_datatable.html#eda",
    "href": "dtool_datatable.html#eda",
    "title": "6  Analysis",
    "section": "6.5 EDA",
    "text": "6.5 EDA\nFocus on a handful of variables after dropped others.\n\ndf &lt;- df_clean %&gt;% \n  select(-c(contains('date'), invoice_number))\n\n\n# freq table \nwith(df, table(disputed, country_code) %&gt;% addmargins())                      \ntapply(df$invoice_amount, list(df$disputed, df$country_code), median)\n\n# descriptive stats\ndf %&gt;% \n  select(where(is.numeric)) %&gt;% \n  summary()\n\n\n# normal distribution\ndf %&gt;% \n  ggplot(aes(invoice_amount, fill = disputed)) +\n  geom_histogram(bins = 10, position = 'dodge') +\n  geom_vline(xintercept = median(df$invoice_amount), color = 'red', \n             size = 3, linetype = \"dashed\") +\n  theme_light()\n\n\n# correlation\ndf %&gt;% \n  select(where(is.numeric)) %&gt;% \n  cor() %&gt;% \n  corrplot::corrplot(method = 'color', order = 'FPC', type = 'lower', diag = FALSE)\n\ndf %&gt;% \n  select(where(is.numeric)) %&gt;% \n  corrr::correlate() %&gt;% \n  corrr::rearrange() %&gt;% \n  corrr::shave() %&gt;% \n  corrr::fashion()"
  },
  {
    "objectID": "dtool_datatable.html#model",
    "href": "dtool_datatable.html#model",
    "title": "6  Analysis",
    "section": "6.6 Model",
    "text": "6.6 Model\nRead more about logistic regression here, here, and here.\n\n# easy stats plot\ndf %&gt;%\n  mutate(prob = ifelse(disputed == \"Yes\", 1, 0)) %&gt;%\n  ggplot(aes(late, prob)) +\n  geom_point(alpha = .2) +\n  geom_smooth(method = \"glm\", method.args = list(family = \"binomial\")) +\n  theme_light()\n\n\n# model comparison\ndf_mod &lt;- df %&gt;%\n  mutate(disputed = as.factor(disputed))\n\nmod1 &lt;- glm(disputed ~ late, family = \"binomial\", data = df_mod)\nmod2 &lt;- glm(disputed ~ late + settle + invoice_amount, \n            family = \"binomial\", data = df_mod)\n\nsummary(mod1)\nanova(mod1, mod2, test = \"Chisq\")\n\n\n# model diagnostic\ndf_mod_res &lt;- broom::augment(mod1, df_mod) %&gt;% \n  mutate(pred = ifelse(.fitted  &gt; .5, \"Yes\", \"No\") %&gt;% as.factor())\n\n# confusion matrix  \ndf_mod_res %&gt;% \n  yardstick::conf_mat(disputed, pred) %&gt;%\n  autoplot()\n\n# plot pred\ndf_mod_res %&gt;% \n  mutate(res = disputed == pred) %&gt;% \n  ggplot(aes(invoice_amount, settle, color = res)) +\n  geom_point() +\n  theme_light()\n\ndf_mod_res %&gt;% \n  ggplot(aes(invoice_amount, settle, color = disputed)) + \n  geom_point() + \n  facet_wrap(~pred) +\n  theme_light()"
  },
  {
    "objectID": "dtool_datatable.html#report",
    "href": "dtool_datatable.html#report",
    "title": "6  Analysis",
    "section": "6.7 Report",
    "text": "6.7 Report\n\nlibrary(patchwork)\nlibrary(ggtext)\nlibrary(showtext)\n\np1 &lt;- df %&gt;% \n  ggplot(aes(invoice_amount, settle, color = disputed)) +\n  geom_point() +\n  scale_color_manual(labels = c(\"Agreed\", 'Disputed'), \n                     values = c(\"#9AC2BB\", '#E99184')) +\n  guides(color = guide_legend(title.position = \"top\", title = \"\")) +\n  labs(x = \"\", y = \"Settlement days\") +\n  theme_light() +\n  theme(\n    legend.position = c(.95, .98), \n    legend.background = element_rect(color = \"transparent\", fill = 'transparent'),\n    legend.box.background = element_rect(color = \"transparent\", fill = \"transparent\"),\n    legend.key = element_rect(colour = \"transparent\", fill = \"transparent\")\n  )\n\np2 &lt;- df %&gt;% \n  group_by(if_late = late == 0) %&gt;% \n  ggplot(aes(invoice_amount, settle, color = disputed)) +\n  geom_point(show.legend = FALSE) +\n  scale_color_manual(labels = c(\"Agreed\", 'Disputed'), \n                     values = c(\"#9AC2BB\", '#E99184')) +\n  facet_wrap(~if_late) +\n  labs(caption = \"©RAudit Solution | **Stewart Li**&lt;br&gt;(Data source: Kaggle)\", \n       x = \"Invoice amount\", \n       y = \"Settlement days\") +\n  theme_light() +\n  theme(\n    axis.title.y = element_text(margin = margin(b = 1, unit = \"in\")),\n    strip.text = element_text(color = '#2D4248'),\n    strip.background = element_blank(), \n    plot.caption = element_markdown(lineheight = 1.2)\n  )\n\np1 / p2 +\n  plot_annotation(\n    title = \"The &lt;span style = 'color:#E99184;'&gt;Analysis&lt;/span&gt; of cash collection\", \n    subtitle = 'Focus on those slow settlement without dispute', \n    tag_levels = 'A'\n  ) &\n  theme(plot.tag = element_text(size = 8), \n        plot.title = element_markdown())\n\n\n\n\nCombined plot"
  },
  {
    "objectID": "dtool_payroll.html#cleaning",
    "href": "dtool_payroll.html#cleaning",
    "title": "7  Audit",
    "section": "7.1 Cleaning",
    "text": "7.1 Cleaning\n\nexp_claim_raw &lt;- readxl::read_excel(\"isca_cpe_2023/1. Anomalies in Payroll data.xlsx\", \n                                    sheet = 1, \n                                    range = \"A1:G33\") %&gt;% \n  janitor::clean_names()\n\nhr_data_raw &lt;- readxl::read_excel(\"isca_cpe_2023/1. Anomalies in Payroll data.xlsx\", \n                                  sheet = 2) %&gt;% \n  janitor::clean_names()\n\npay_data_raw &lt;- readxl::read_excel(\"isca_cpe_2023/1. Anomalies in Payroll data.xlsx\", \n                                   sheet = 3, \n                                   skip = 2, range = \"A3:D25\") %&gt;% \n  janitor::clean_names()\n\ndf_comb &lt;- exp_claim_raw %&gt;% \n  full_join(hr_data_raw, by = c('staff_id' = 'staff_id')) %&gt;% \n  left_join(pay_data_raw, by = c('staff_id' = 'staff_id'))\n\n\ndf_clean &lt;- df_comb %&gt;% \n  mutate(across(contains(\"date\"), lubridate::dmy)) %&gt;% \n  mutate(on_leave = lubridate::dmy(on_leave)) %&gt;% \n  mutate(staff_name = coalesce(staff_name, name.x)) \n\n# check if amount is correct\nsum(df_clean$amount_s.x, na.rm = TRUE)\n\ndf_clean %&gt;% \n  distinct(staff_id, amount_s.y) %&gt;% \n  summarise(app_c = sum(amount_s.y, na.rm = TRUE))\n\n\nsheets &lt;- list(\"comb\" = df_comb, \"clean\" = df_clean) \nwritexl::write_xlsx(sheets, here::here(paste0('audit_sit/audit_payroll', Sys.Date(), '.xlsx')))\nopenxlsx::openXL(here::here(\"audit_sit/audit_payroll2023-12-22.xlsx\")) \n\ndf_clean &lt;- readxl::read_excel(here::here(\"audit_sit/audit_payroll2023-12-22.xlsx\")) %&gt;% \n  mutate(across(c(contains(\"date\"), on_leave), lubridate::dmy))"
  },
  {
    "objectID": "dtool_payroll.html#procedure",
    "href": "dtool_payroll.html#procedure",
    "title": "7  Audit",
    "section": "7.2 Procedure",
    "text": "7.2 Procedure\n\n# cross check payroll amount against finance amount\ndf_clean %&gt;% \n  group_by(staff_id, staff_name) %&gt;% \n  summarise(amt_exp = sum(amount_s.x), \n            amt_paid = sum(amount_s.y) / n(), \n            amt_diff = amt_exp - amt_paid, \n            .groups = 'drop') \n\n\n# compare date to ensure no claim happens before incurred or after resigned\ndf_clean %&gt;% \n  dplyr::filter(claim_date &gt; expense_date) \n\ndf_clean %&gt;% \n  dplyr::filter(claim_date &gt; last_date | claim_date == on_leave) \n\n\n# identify multiple claims for the same expense\ndf_clean %&gt;% \n  group_by(staff_id, staff_name, purpose, amount_s.x) %&gt;% \n  dplyr::filter(n() &gt; 1) \n\n\n# ensure staff name and their bank account number updated timely\ndf_clean %&gt;% \n  dplyr::filter(!is.na(edits_to_hr_data), \n                bank_account_no.x == bank_account_no.y) \n\ndf_clean %&gt;% \n  dplyr::filter(name.x != name.y)\n\n\n# produces audit working paper\nlibrary(pointblank)\n\nag &lt;- df_clean %&gt;% \n  create_agent(label = \"A very *simple* example.\", tbl_name = \"payroll\") %&gt;%\n  col_vals_between(columns = claim_date, left = vars(expense_date), right = vars(last_date)) %&gt;%\n  interrogate()\n\nag\n\n\n\n\nAudit Procedure 1\n\n\n\n\n\nAudit Procedure 2"
  },
  {
    "objectID": "dtool_payroll.html#enhanced",
    "href": "dtool_payroll.html#enhanced",
    "title": "7  Audit",
    "section": "7.3 Enhanced",
    "text": "7.3 Enhanced\n\ndf_clean %&gt;% \n  count(staff_name, sort = TRUE)\n\ndf_clean %&gt;% \n  dplyr::filter(grepl(\"\\\\d+?\", purpose)) %&gt;% \n  mutate(purpose = gsub(\"\\\\d+?\", \"\", purpose)) %&gt;% \n  mutate(across(where(is.character), ~na_if(., \"AB99\"))) %&gt;% \n  mutate(staff_id = replace_na(staff_id, 0)) \n\ndf_clean %&gt;% \n  select(contains(\"date\"), purpose) %&gt;% \n  mutate(if_taxi = case_when(str_detect(purpose, \"Taxi\") ~ \"taxi\", \n                             TRUE ~ \"other\"),   \n         total_date = lubridate::floor_date(claim_date, \"week\"), \n         first_date = first(total_date)) %&gt;% \n  slice_max(order_by = claim_date, n = 3)\n\ndf_clean %&gt;% \n  dplyr::filter(!is.na(amount_s.x)) %&gt;% \n  mutate(new = (amount_s.x %/% 100) * 100) %&gt;% \n  group_by(new, amount_s.x &gt; 300) %&gt;% \n  summarise(new1 = mean(amount_s.x), .groups = 'drop')\n\n\ndf_clean %&gt;% \n  dplyr::filter(!is.na(staff_name)) %&gt;% \n  group_nest(staff_id, staff_name) %&gt;% \n  mutate(new = map(data, ~pluck(.x, 4))) %&gt;%\n  mutate(new1 = map(new, ~paste(.x, collapse = '|'))) %&gt;%\n  select(-data, -new) %&gt;% \n  unnest(new1)\n\ndf_clean %&gt;% \n  dplyr::filter(!is.na(staff_name)) %&gt;% \n  select(staff_id, staff_name, purpose) %&gt;% \n  summarise(new1 = paste(purpose, collapse = '|'), .by = c(staff_id, staff_name)) \n\n\ndf_clean %&gt;% \n  select(staff_id, staff_name, division, purpose, amount_s.x) %&gt;% \n  dplyr::filter(!is.na(purpose)) %&gt;% \n  separate(purpose, into = c(\"type\", \"info\"), \n           extra = 'merge', remove = FALSE, fill = 'right') %&gt;% \n  group_by(division, type) %&gt;% \n  summarise(n = n(), \n            amt_type = sum(amount_s.x), .groups = 'drop') %&gt;% \n  arrange(-amt_type)\n\n\nlibrary(lubridate)\n\ndf_clean %&gt;% \n  pivot_longer(cols = where(is.Date), \n               names_to = 'activity_date', \n               values_to = 'detail_date', \n               names_pattern = \"(.*)_.*\", \n               names_transform = list(activity_date = toupper))"
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "References",
    "section": "",
    "text": "Li, Stewart, Richard Fisher, and Michael Falta. 2020. “The\nEffectiveness of Artificial Neural Networks Applied to Analytical\nProcedures Using High Level Data: A Simulation Analysis.”\nMeditari Accountancy Research 29 (6): 1425–50. https://doi.org/10.1108/medar-06-2020-0920.\n\n\nWickham, Hadley. 2016. Ggplot2: Elegant Graphics for Data\nAnalysis. Springer-Verlag New York. https://ggplot2.tidyverse.org."
  }
]